# Usage-Based Rate Limiting with Envoy AI Gateway

This demo showcases usage-based rate limiting for AI workloads, allowing you to control token consumption per user and model. It builds on top of the 01-getting-started demo environment.

## Overview

This demo demonstrates:
- **Token-based rate limiting** for LLM requests
- **Per-user and per-model limits** with different token quotas
- **Automatic token tracking** from LLM responses
- **Rate limit enforcement** with 429 status codes when limits are exceeded

## Prerequisites

- Completed setup of the 01-getting-started demo
- Kubernetes cluster with Envoy AI Gateway installed
- `kubectl` configured to access your cluster
- `task` (Taskfile) installed
- `jq` for JSON pretty-printing

## Quick Start

1. Navigate to the demo directory:
```bash
cd demos/02-usage-based-rate-limiting
```

2. Deploy the rate limiting configuration:
```bash
task setup
```

This will:
- Ensure the basic environment from 01-getting-started is running
- Deploy the BackendTrafficPolicy for rate limiting
- Replace the AIGatewayRoute with an enhanced version that includes token tracking
- Enable support for additional models (gpt-4, gpt-3.5-turbo)

3. Start port forwarding (if not already running):
```bash
task port-forward
```

4. Test the rate limiting:
```bash
task test-rate-limits
```

## Configuration Details

### Token Tracking

The configuration tracks three types of tokens:
- **Input tokens**: Tokens from the user's prompt
- **Output tokens**: Tokens generated by the model
- **Total tokens**: Combined input and output tokens

```yaml
llmRequestCosts:
  - metadataKey: llm_input_token
    type: InputToken
  - metadataKey: llm_output_token
    type: OutputToken
```

### Rate Limits by Model

Different models have different token limits per hour:
- **gpt-4**: 1,000 tokens/hour per user
- **gpt-3.5-turbo**: 5,000 tokens/hour per user
- **qwen3**: 2,000 tokens/hour per user

### User Identification

Rate limits are applied per user using the `x-user-id` header:
```bash
curl -H "x-user-id: alice" -H "x-ai-eg-model: gpt-4" ...
```

## Testing Examples

### Test Different Users
```bash
# Alice using qwen3
curl -H "Content-Type: application/json" \
     -H "x-user-id: alice" \
     -H "x-ai-eg-model: qwen3" \
     -d '{
       "model": "qwen3",
       "messages": [{"role": "user", "content": "Hello from Alice!"}]
     }' \
     http://localhost:8080/v1/chat/completions

# Bob using gpt-3.5-turbo
curl -H "Content-Type: application/json" \
     -H "x-user-id: bob" \
     -H "x-ai-eg-model: gpt-3.5-turbo" \
     -d '{
       "model": "gpt-3.5-turbo",
       "messages": [{"role": "user", "content": "Hello from Bob!"}]
     }' \
     http://localhost:8080/v1/chat/completions
```

### Test Rate Limit Exceeded
```bash
# Send multiple requests to trigger rate limit
task test-limits-exceeded
```

When the rate limit is exceeded, you'll receive a 429 status code:
```json
{
  "error": {
    "message": "Rate limit exceeded",
    "type": "rate_limit_error",
    "code": 429
  }
}
```

## Available Tasks

- `task setup` - Deploy rate limiting configuration
- `task test-rate-limits` - Run all rate limiting tests
- `task test-user-alice` - Test limits for user Alice
- `task test-user-bob` - Test limits for user Bob
- `task test-limits-exceeded` - Test rate limit enforcement
- `task test-streaming` - Test streaming with rate limits
- `task monitor-rates` - Check current rate limit statistics
- `task status` - View rate limiting configuration
- `task logs` - Show rate limit related logs
- `task cleanup` - Remove rate limiting configuration

## How It Works

1. **Route Enhancement**: Demo 02 replaces the AIGatewayRoute from demo 01:
   - Adds `llmRequestCosts` configuration for token tracking
   - Extends support to additional models while preserving the original qwen3 route

2. **Request Processing**: When a request arrives, the gateway extracts:
   - User ID from `x-user-id` header
   - Model from `x-ai-eg-model` header

3. **Token Tracking**: The AI Gateway automatically:
   - Extracts token usage from LLM responses
   - Updates the user's token consumption

4. **Rate Limit Check**: Before processing:
   - Checks if the user has remaining tokens
   - Rejects requests that would exceed limits

5. **Response**: 
   - Successful requests are processed normally
   - Rate-limited requests receive 429 status

## Architecture

```
┌─────────────┐     ┌─────────────────┐     ┌──────────────────┐
│   Client    │────▶│  Envoy AI       │────▶│  LLM Backend    │
│ (w/ user-id)│     │  Gateway        │     │                 │
└─────────────┘     └─────────────────┘     └──────────────────┘
                           │
                           ▼
                    ┌─────────────────┐
                    │  Rate Limiter   │
                    │ (Token-based)   │
                    └─────────────────┘
```

## Troubleshooting

### Rate Limits Not Working
1. Ensure you're including the `x-user-id` header
2. Check that the model name in `x-ai-eg-model` matches configuration
3. Verify the BackendTrafficPolicy is applied: `task status`

### 429 Errors on First Request
- The rate limiter may have residual state from previous tests
- Wait a few minutes or use a different user ID

### Token Counts Not Accurate
- The LLM-D simulator returns mock token counts
- With real LLM backends, token counts will be accurate

## Advanced Configuration

### Custom Token Costs

You can implement custom token cost calculations using CEL expressions:
```yaml
llmRequestCosts:
  - metadataKey: custom_cost
    celExpression: "has(llm_input_token) ? llm_input_token * 2 : 0"
```

### Different Time Windows

Modify the rate limit time window:
```yaml
limit:
  requests: 1000
  unit: Minute  # or Hour, Day
```

## Cleanup

To remove the rate limiting configuration:
```bash
task cleanup
```

This will:
- Remove the BackendTrafficPolicy
- Revert the AIGatewayRoute to its original demo 01 configuration
- Preserve the base environment from 01-getting-started

## Next Steps

- Implement different rate limits for different user tiers
- Add token cost calculations based on model pricing
- Configure rate limiting with Redis for distributed deployments
- Combine with authentication for production use

## References

- [Usage-Based Rate Limiting Documentation](https://aigateway.envoyproxy.io/docs/capabilities/traffic/usage-based-ratelimiting)
- [Envoy Rate Limiting](https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/rate_limit_filter)
- [BackendTrafficPolicy API](https://gateway.envoyproxy.io/docs/api/extension_types/#backendtrafficpolicy)